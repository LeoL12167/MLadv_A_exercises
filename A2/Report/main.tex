\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage[T1]{fontenc}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}
\usepackage[sorting=none]{biblatex}
\usepackage{fancyhdr}
\usepackage{multicol}
\usepackage{enumitem}
\addbibresource{ref.bib}
\setlength{\columnsep}{40pt}
\setlength{\voffset}{0.7cm}
\setlength{\headsep}{40pt}
\geometry{legalpaper, portrait, margin=2cm}


% Title page
\title{HW A2\\\Large{Machine Learning, Advanced Course/DD2434/mladv24}}
\author{Aurhor \\ KTH Royal Institute of Technology\\ School of Engineering Sciences in Chemistry, Biotechnology and Health}
\date{\today}

% Header and footer
\pagestyle{fancy}
\fancyhead{}
\fancyhead[L]{\textbf{Machine Learning, Advanced Course}\\\textbf{DD2434}}
\fancyhead[R]{\textbf{Andrea Stanziale, Leonardo LÃ¼der}\\ stanz@kth.se, luder@kth.se}
\fancyfoot{}
\begin{document}

\maketitle

% Begin page numbers
\fancyfoot[C]{\thepage}
\pagenumbering{arabic}
\begin{multicols}{2}

    \section*{2.1}
    The answers to the questions are as follows:
    \begin{enumerate}[noitemsep, topsep=0pt]
        \item Bernoulli
        \item Gamma
        \item Log-normal
        \item Beta
    \end{enumerate}
    \section*{2.2}
    \subsection*{2.2.5} 
    The definition given of the local hidden variables is that they must be dependent
    only on other local variables of the local context and the global variables, that is:
    \begin{align}
        p(z_{d}|\theta_{-d}, \beta, \alpha, w_{-d}) = p(z_{d} | \beta, \alpha)
    \end{align}
    \subsection*{2.2.6}





    \subsection*{2.2.7}

    The ELBO for the LDA is defined, according to Blei et al. \cite{blei2009visualizing} as:
    
    \begin{equation}
        \begin{aligned}
            \mathcal{L} &= \sum_{k=1}^{K} \mathbb{E}[\log p(\beta_k | \eta)]+\sum_{d=1}^{D} \mathbb{E}[\log p(\theta_d | \alpha)] \\
            &+ \sum_{d=1}^{D} \sum_{n=1}^{N} \mathbb{E}[\log p(z_{d,n} | \theta_d)] \\
            &+ \sum_{z=1}^{Z} \sum_{n=1}^{N} \mathbb{E}[\log p(w_{d,n} | z_{d,n}, \beta_{1:K})] + \mathcal{H}(\mathbf{g})
        \end{aligned}
    \end{equation}

    \subsection*{2.2.8}

    The runtime of the SVI algorithm was found to be around 1.5 to 3 times faster than the CAVI updates, on the flip side, the ELBO was lower
    than the one of the CAVI algorithm, the values of $E[\theta]$ and $E[\beta]$ were also generally closer for the CAVI algorithm.    


\section*{2.3 BBVI}
In BBVI without Rao-Blackwellization and control variates, the gradient is estimated using Monte Carlo sampling, the score function of \(q\), and the joint of \(p\).

\subsection*{Question 2.3.9}
Let \(X = (X_1, \dots, X_N)\) be i.i.d.\ with \(X_n | \theta, \sigma^2 \sim \mathcal{N}(\theta, \sigma^2)\), \(\theta \sim \text{LogNormal}(\nu, \epsilon^2)\), and \(\sigma^2\) fixed. Write an expression for the Naive BBVI gradient estimate w.r.t.\ \(\alpha\) using one sample \(z \sim q(\theta)\), \(q(\theta) = \text{Gamma}(\alpha, \beta)\). (2 points)\bigskip 

We write the ELBO as:
\begin{align*}
    \mathbb{L} = \mathbb{E}_{q(\theta)}\left[\log p(X, \theta) - \log q(\theta)\right].
\end{align*}
We compute the gradient of the ELBO w.r.t.\ \(\alpha\) using the score function estimator:
\begin{align*}
    \nabla_\alpha \mathbb{L} &= \mathbb{E}_{q(\theta)}\left[(\log p(X, \theta) - \log q(\theta)) \nabla_\alpha \log q(\theta) \right].
\end{align*}
We take the Monte Carlo estimate of the gradient:
\begin{align*}
    \nabla_\alpha \mathbb{L} &\approx \widehat{\nabla_\alpha \mathbb{L}} = \frac{1}{S}\sum_{s=1}^{S}(\log p(X, z_s) - \log q(z_s)) \nabla_\alpha \log q(z_s),
\end{align*}
where \(z_s \sim q(\theta)\). As we take one sample only, this expression simplifies to:
\begin{align*}
    \widehat{\nabla_\alpha \mathbb{L}} = (\log p(X, z_1) - \log q(z_1)) \nabla_\alpha \log q(z_1),
\end{align*}
with \(z_1 \sim q(\theta)\).

For the variational distribution \(q(\theta)\), with \(q(\theta) = \text{Gamma}(\alpha, \beta)\), we have:
\begin{align*}
    \log q(\theta) &= (\alpha - 1)\log \theta - \theta \beta - \log \Gamma(\alpha) + \alpha \log \beta, \\
    \nabla_\alpha \log q(\theta) &= \log(\theta) - \psi(\alpha) + \log(\beta),
\end{align*}
where \(\psi(\alpha)\) is the digamma function.

For the log-joint \(\log p(X, \theta)\), we have:
\begin{align*}
    \log p(X, \theta) &= \sum_{n=1}^{N} \log p(X_n \mid \theta) + \log p(\theta), \\
    \log p(X_n \mid \theta) &= -\frac{1}{2} \log(2\pi\sigma^2) - \frac{(X_n - \theta)^2}{2\sigma^2}, \\
    \log p(\theta) &= -\log(\theta \epsilon \sqrt{2\pi}) - \frac{(\log \theta - \nu)^2}{2\epsilon^2}.
\end{align*}

The Monte Carlo estimate of the gradient becomes:
\begin{align*}
    \widehat{\nabla_\alpha \mathbb{L}} &= \Bigg[\sum_{n=1}^N \Big(-\frac{(X_n - \theta)^2}{2\sigma^2} - \frac{1}{2} \log(2\pi\sigma^2)\Big) \\&
    - \log(\theta \epsilon \sqrt{2\pi}) - \frac{(\log(\theta) - \nu)^2}{2\epsilon^2} \\
    &- (\alpha - 1)\log \theta + \theta \beta + \log \Gamma(\alpha) - \alpha \log \beta \Bigg]\\& \cdot (\log \theta - \psi(\alpha) + \log \beta).
\end{align*}

\subsection*{Question 2.3.10}
Describe in one sentence what Rao-Blackwellization is used for in the BBVI paper. (1 point)\bigskip 

In the BBVI paper, Rao-Blackwellization is used to reduce the variance of the random variables used to estimate the ELBO's gradient by replacing them with their conditional expectations given variables outside of their Markov blanket.


\subsection*{Question 2.3.11}
Given the model in figure 2, and the mean-field approximation:  
\[
q(y, w, z, v) = q_\lambda^1(w)q_\lambda^2(z)q_\lambda^3(v)\prod_n q_\lambda^4(y_n),
\]
describe qualitatively how the Rao-Blackwellized partial gradient of the ELBO w.r.t. \(\lambda_n^4\), \(\nabla_{\lambda_n^4} \mathcal{L}\), is obtained. Write out the final expression for the Rao-Blackwellized \(\nabla_{\lambda_n^4} \mathcal{L}\). (2 points)







\end{multicols}

\clearpage
\addcontentsline{toc}{section}{References}
\printbibliography{}

\end{document}
