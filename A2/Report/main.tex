\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage[T1]{fontenc}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}
\usepackage[sorting=none]{biblatex}
\usepackage{fancyhdr}
\usepackage{multicol}
\usepackage{enumitem}
\addbibresource{ref.bib}
\setlength{\columnsep}{40pt}
\setlength{\voffset}{0.7cm}
\setlength{\headsep}{40pt}
\geometry{legalpaper, portrait, margin=2cm}


% Title page
\title{HW A2\\\Large{Machine Learning, Advanced Course/DD2434/mladv24}}
\author{Aurhor \\ KTH Royal Institute of Technology\\ School of Engineering Sciences in Chemistry, Biotechnology and Health}
\date{\today}

% Header and footer
\pagestyle{fancy}
\fancyhead{}
\fancyhead[L]{\textbf{Machine Learning, Advanced Course}\\\textbf{DD2434}}
\fancyhead[R]{\textbf{Andrea Stanziale, Leonardo LÃ¼der}\\ stanz@kth.se, luder@kth.se}
\fancyfoot{}
\begin{document}

\maketitle

% Begin page numbers
\fancyfoot[C]{\thepage}
\pagenumbering{arabic}
\begin{multicols}{2}

    \section*{2.1}
    The answers to the questions are as follows:

    \subsection*{2.1.1}
    $p(x|\theta) = \text{exp}(\log \frac{p}{1-p} x -\log(frac{1}{1-p}))$ which simplifies to $p(x|\theta) =p^x(1-p)^{1-x}$
    which is the Bernoulli distribution.

    \subsection*{2.1.2}
    $p(x|\theta) = \text{exp}((\alpha-1)\log x - \beta x - \log \Gamma (\alpha)-(\alpha)\log(\beta))$ which corresponds to
    $p(x|\theta) = x^{\alpha-1} e^{-\beta x} \Gamma(\alpha)^{-1}\beta^{-\alpha}$  which is the Gamma distribution.

    \subsection*{2.1.3}
    $p(x|\theta) = \frac{1}{x \sqrt{2\pi}} \exp (\frac{\mu}{\sigma^2} \log x - \frac{1}{2\sigma^2} (\log x)^2 + \frac{\mu^2}{2 \sigma^2} - \frac{1}{2} \log (\frac{1}{\sigma^2})) $ this simplifies to
    \begin{equation*}
        \begin{aligned}
            p(x|\theta) &= \frac{1}{\sigma x \sqrt{2\pi}} \cdot \\
            &\exp {-\frac{(\log x - \mu)^2}{2 \sigma^2}}
        \end{aligned}    
    \end{equation*}
    which is the log-normal distribution.

    \subsection*{2.1.4}
    $p(x|\theta) = \exp( (\psi_1 - 1) \log x + (\psi_2-1)\log(1-x) - log\Gamma(\psi_1)-\log(\Gamma(\psi_2))+\log \Gamma(\psi_1+\psi_2))$
    whixh can be written as $p(x|\theta) = x^{\psi_1-1}(1-x)^{\psi_2-1} \Gamma(\psi_1)\Gamma(\psi_2)\Gamma^{-1}(\psi_1+\psi_2)$ which
    represents a beta distribution.
    \section*{2.2}
    \subsection*{2.2.5} 
    The definition given of the local hidden variables is that they must be dependent
    only on other local variables of the local context and the global variables, that is:
    \begin{align}
        p(z_{d}|\theta_{-d}, \beta, \alpha, w_{-d}) = p(z_{d} | \beta, \alpha)
    \end{align}
    \subsection*{2.2.6}
    Consider the LDA model in 1. Let \(w_d = w_{d,1:N}\) and \(z_d = z_{d,1:N}\). Show that \(\theta_d, z_d\) fulfills the definition in 2.2.5. (1 point)\bigskip

    We treat $K$ as a fixed parameter, and we have the following joint probability distribution:
    \[
    p(w, z, \theta | \alpha, \beta) = p(\theta | \alpha) p(z | \theta) p(w | z, \beta).
    \]

    We decompose the terms, by using, that:
    \begin{align*}
    p(z | \theta) &= \prod_{d=1}^D \prod_{n=1}^N p(z_{d,n} | \theta_d), \\
    p(z | \theta) &= \prod_{d=1}^D \prod_{n=1}^N p(z_{d,n} | \theta_d), \\
    p(w | z, \beta) &= \prod_{d=1}^D \prod_{n=1}^N p(w_{d,n} | z_{d,n}, \beta).
    \end{align*}

    We find the factorized joint distribution as:

    \[
    p(w, z, \theta | \alpha, \beta) = \prod_{d=1}^D \Big[ p(\theta_d | \alpha) \prod_{n=1}^N p(z_{d,n} | \theta_d) p(w_{d,n} | z_{d,n}, \beta) \Big].
    \]

    We see, that given the global variables $\alpha$ and $\beta$, the local hidden variable $z_{d,n}$ depends only on $\theta_d$ and $\beta$, and the global hidden variable $\theta_d$ depends only on $\alpha$.
    We therefore have:
    \[
    p(z_{d} |\theta_{-d}, \beta, \alpha, w_{-d}) = p(z_{d} |\theta_d, \alpha, \beta),
    \]
    and therefore fulfilling the definition of 2.2.5.





\subsection*{2.2.7}
Write the ELBO for the LDA model as a function of variational parameters, \(\phi_{dn}, \gamma_d, \lambda_k\), prior parameters, \(\alpha, \eta\), and hyperparameters \(D, N, K, W\). No derivation is needed, only the final expression. You may use an online source for this derivation, in which case you must provide a link to the source. (2 points)\bigskip

The ELBO for the LDA is defined, according to Blei et al. \cite{blei2009visualizing} as:

\begin{equation}
    \begin{aligned}
        \mathcal{L} &= \sum_{k=1}^{K} \mathbb{E}[\log p(\beta_k | \eta)]+\sum_{d=1}^{D} \mathbb{E}[\log p(\theta_d | \alpha)] \\
        &+ \sum_{d=1}^{D} \sum_{n=1}^{N} \mathbb{E}[\log p(z_{d,n} | \theta_d)] \\
        &+ \sum_{z=1}^{Z} \sum_{n=1}^{N} \mathbb{E}[\log p(w_{d,n} | z_{d,n}, \beta_{1:K})] + \mathcal{H}(\mathbf{g})
    \end{aligned}
\end{equation}

\subsection*{2.2.8}
Adjust the CAVI updates provided in the notebook to SVI updates and implement the SVI algorithm. Use the function provided for generating data and run the algorithm for the cases defined in the notebook. In one sentence, comment on the success and runtime of each experiment. (7 points)\bigskip

The runtime of the SVI algorithm was found to be around 1.5 to 3 times faster than the CAVI updates, on the flip side, the ELBO was lower
than the one of the CAVI algorithm, the values of $E[\theta]$ and $E[\beta]$ were also generally closer for the CAVI algorithm.    


\section*{2.3 BBVI}
    In BBVI without Rao-Blackwellization and control variates, the gradient is estimated using Monte Carlo sampling, the score function of \(q\), and the joint of \(p\).

    \subsection*{Question 2.3.9}
    Let \(X = (X_1, \dots, X_N)\) be i.i.d.\ with \(X_n | \theta, \sigma^2 \sim \mathcal{N}(\theta, \sigma^2)\), \(\theta \sim \text{LogNormal}(\nu, \epsilon^2)\), and \(\sigma^2\) fixed. Write an expression for the Naive BBVI gradient estimate w.r.t.\ \(\alpha\) using one sample \(z \sim q(\theta)\), \(q(\theta) = \text{Gamma}(\alpha, \beta)\). (2 points)\bigskip 

    We write the ELBO as:
    \begin{align*}
        \mathbb{L} = \mathbb{E}_{q(\theta)}\left[\log p(X, \theta) - \log q(\theta)\right].
    \end{align*}
    We compute the gradient of the ELBO w.r.t.\ \(\alpha\) using the score function estimator:
    \begin{align*}
        \nabla_\alpha \mathbb{L} &= \mathbb{E}_{q(\theta)}\left[(\log p(X, \theta) - \log q(\theta)) \nabla_\alpha \log q(\theta) \right].
    \end{align*}
    We take the Monte Carlo estimate of the gradient:
    \begin{align*}
        \nabla_\alpha \mathbb{L} &\approx \widehat{\nabla_\alpha \mathbb{L}} = \frac{1}{S}\sum_{s=1}^{S}(\log p(X, z_s) - \log q(z_s)) \nabla_\alpha \log q(z_s),
    \end{align*}
    where \(z_s \sim q(\theta)\). As we take one sample only, this expression simplifies to:
    \begin{align*}
        \widehat{\nabla_\alpha \mathbb{L}} = (\log p(X, z_1) - \log q(z_1)) \nabla_\alpha \log q(z_1),
    \end{align*}
    with \(z_1 \sim q(\theta)\).

    For the variational distribution \(q(\theta)\), with \(q(\theta) = \text{Gamma}(\alpha, \beta)\), we have:
    \begin{align*}
        \log q(\theta) &= (\alpha - 1)\log \theta - \theta \beta - \log \Gamma(\alpha) + \alpha \log \beta, \\
        \nabla_\alpha \log q(\theta) &= \log(\theta) - \psi(\alpha) + \log(\beta),
    \end{align*}
    where \(\psi(\alpha)\) is the digamma function.

    For the log-joint \(\log p(X, \theta)\), we have:
    \begin{align*}
        \log p(X, \theta) &= \sum_{n=1}^{N} \log p(X_n \mid \theta) + \log p(\theta), \\
        \log p(X_n \mid \theta) &= -\frac{1}{2} \log(2\pi\sigma^2) - \frac{(X_n - \theta)^2}{2\sigma^2}, \\
        \log p(\theta) &= -\log(\theta \epsilon \sqrt{2\pi}) - \frac{(\log \theta - \nu)^2}{2\epsilon^2}.
    \end{align*}

    The Monte Carlo estimate of the gradient becomes:
    \begin{align*}
        \widehat{\nabla_\alpha \mathbb{L}} &= \Bigg[\sum_{n=1}^N \Big(-\frac{(X_n - \theta)^2}{2\sigma^2} - \frac{1}{2} \log(2\pi\sigma^2)\Big) \\&
        - \log(\theta \epsilon \sqrt{2\pi}) - \frac{(\log(\theta) - \nu)^2}{2\epsilon^2} \\
        &- (\alpha - 1)\log \theta + \theta \beta + \log \Gamma(\alpha) - \alpha \log \beta \Bigg]\\& \cdot (\log \theta - \psi(\alpha) + \log \beta).
    \end{align*}

    \subsection*{Question 2.3.10}
    Describe in one sentence what Rao-Blackwellization is used for in the BBVI paper. (1 point)\bigskip 

    In the BBVI paper, Rao-Blackwellization is used to reduce the variance of the random variables used to estimate the ELBO's gradient by replacing them with their conditional expectations given variables outside of their Markov blanket.


    \subsection*{Question 2.3.11}
    Given the model in figure 2, and the mean-field approximation:  
    \[
    q(y, w, z, v) = q_\lambda^1(w)q_\lambda^2(z)q_\lambda^3(v)\prod_n q_\lambda^4(y_n),
    \]
    describe qualitatively how the Rao-Blackwellized partial gradient of the ELBO w.r.t. \(\lambda_n^4\), \(\nabla_{\lambda_n^4} \mathcal{L}\), is obtained. Write out the final expression for the Rao-Blackwellized \(\nabla_{\lambda_n^4} \mathcal{L}\). (2 points)\bigskip

    We have the ELBO as:
    \begin{align*}
        \mathcal{L} = \mathbb{E}_{q_\lambda}\left[\log p(y, w, z, v) - \log q_\lambda(y, w, z, v)\right].
    \end{align*}
    and its gradient with respect to \(\lambda_n^4\):
    \begin{align*}
        \nabla_{\lambda_n^4}\mathcal{L} = \mathbb{E}_{q_\lambda}\left[\nabla_{\lambda_4}(\log p(y, w, z, v) - \log q_\lambda(y, w, z, v))\right].
    \end{align*}

    with the score function form:

    \begin{align*}
        \nabla_{\lambda_n^4}\mathcal{L} =& \mathbb{E}_{q_\lambda(w,z,v,y_n)}\left[(\log p(y_n|w,z,v)-\right.\\&\left.\log q_{\lambda_n^4}(y_n))\,\nabla_{\lambda_n^4}\log q_{\lambda_n^4}(y_n)\right].
    \end{align*}

    Without Rao-Blackwellization, this would be approximated by sampling $y_n, w,z,v$.
    Instead of sampling $y_n$, we integrate it out analytically, resulting in the final expression:
    \begin{align*}
        \nabla_{\lambda_n^4}\mathcal{L} 
        =& \mathbb{E}_{q_\lambda(w,z,v)}\left[\int q_{\lambda_n^4}(y_n) (\log p(y_n|w,z,v)-\right.\\&\left.\log q_{\lambda_n^4}(y_n)) \nabla_{\lambda_n^4}\log q_{\lambda_n^4}(y_n)\,dy_n\right].
    \end{align*}




\end{multicols}

\clearpage
\addcontentsline{toc}{section}{References}
\printbibliography{}

\end{document}
