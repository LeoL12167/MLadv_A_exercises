\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage[T1]{fontenc}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}
\usepackage[sorting=none]{biblatex}
\usepackage{fancyhdr}
\usepackage{multicol}
\usepackage{enumitem}
\usepackage{tikz}
\usepackage{bm}
\usetikzlibrary{shapes,arrows,fit,positioning}
\addbibresource{ref.bib}
\setlength{\columnsep}{40pt}
\setlength{\voffset}{0.7cm}
\setlength{\headsep}{40pt}
\geometry{legalpaper, portrait, margin=2cm}


% Title page
\title{HW A2\\\Large{Machine Learning, Advanced Course/DD2434/mladv24}}
\author{Andrea Stanziale \\ KTH Royal Institute of Technology}
\date{\today}

% Header and footer
\pagestyle{fancy}
\fancyhead{}
\fancyhead[L]{\textbf{Machine Learning, Advanced Course}\\\textbf{DD2434}}
\fancyhead[R]{\textbf{Andrea Stanziale, Leonardo Lüder}\\ stanz@kth.se}
\fancyfoot{}
\begin{document}

\maketitle

% Begin page numbers
\fancyfoot[C]{\thepage}
\pagenumbering{arabic}
\section*{Exercise 1}
\subsection*{1.1: Bayesian Network}
    \begin{tikzpicture}[>=stealth, node distance=1.5cm,
        latent/.style={circle, draw, thick, minimum size=7mm, inner sep=0pt},
        observed/.style={circle, draw, thick, fill=gray!20, minimum size=7mm, inner sep=0pt},
        plate/.style={rectangle, draw, thick, inner sep=6pt, rounded corners=4pt},
        auto
      ]
      
      % -- Global/Hyperparameter nodes --
      \node[latent] (alpha1) at (0,3) {$\alpha^1$};
      \node[latent, below=of alpha1] (pi1) {$\pi^1$};
      \draw[->] (alpha1) -- (pi1);
      
      \node[latent] (alpha2) at (2,3) {$\alpha^2$};
      \node[latent, below=of alpha2] (pi2) {$\pi^2$};
      \draw[->] (alpha2) -- (pi2);
      
      \node[latent] (beta)  at (6,3) {$\beta$};
      \node[latent, below=of beta]  (rho) {$\rho_1,\dots,\rho_K$};
      \draw[->] (beta) -- (rho);
      
      % -- Per-data variables (Z^1_n, Z^2_n, then X_{n j}) --
      \node[latent]   (Z1) at (0,-1)   {$Z^1_n$};
      \node[latent]   (Z2) at (2,-1)   {$Z^2_n$};
      
      \draw[->] (pi1) -- (Z1);
      \draw[->] (pi2) -- (Z2);
      
      \node[observed] (X)  at (1,-2.5) {$X_{nj}$};
      
      \draw[->] (Z1) -- (X);
      \draw[->] (Z2) -- (X);
      \draw[->] (rho) -- (X);
      
      % -- Plate for j=1..J --
    \node[plate, fit=(X), inner sep=6pt] (plateJ) {};
    \node[above right, xshift=1pt, yshift=-4pt] at (plateJ.north east) {$j=1,\dots,J$};

    % -- Plate for n=1..N (includes Z1, Z2, and the j-plate) --
    \node[plate, fit=(Z1)(Z2)(plateJ), inner sep=8pt] (plateN) {};
    \node[above right, xshift=1pt, yshift=-4pt] at (plateN.north east) {$n=1,\dots,N$};
    \end{tikzpicture}

\subsection*{1.2: Joint Log-Probability}
We want to express the joint log‐probability
\[
\log p\bigl(X,\;Z^1,\;Z^2,\;\pi^1,\;\pi^2,\;\rho 
\;\big|\;\alpha^1,\;\alpha^2,\;\beta \bigr).
\]
By the model factorization:
\begin{equation*}
\begin{aligned}
p(X, Z^1, Z^2, \pi^1, \pi^2, \rho \mid \alpha^1, \alpha^2, \beta) = p(\pi^1 \mid \alpha^1) p(\pi^2 \mid \alpha^2) 
\prod_k p(\rho_k \mid \beta_k) 
\prod_n \Big[ p(Z_n^1 \mid \pi^1) p(Z_n^2 \mid \pi^2) 
\prod_j p(X_{nj} \mid \rho_{k=n}) \Big]
\end{aligned}
\end{equation*}
with log‐probability:
\begin{equation*}
\begin{aligned}
&\log p(X, Z^1, Z^2, \pi^1, \pi^2, \rho \mid \alpha^1, \alpha^2, \beta)  \\
&=
\log p(\pi^1) + \log p(\pi^2) + 
\sum_k \log p(\rho_k) +
\sum_n \Big[
\log p(Z_n^1 \mid \pi^1) +
\log p(Z_n^2 \mid \pi^2) +
\sum_j \log p(X_{nj} \mid \rho_k)
\Big]
\end{aligned}
\end{equation*}
So we write the log‐probability as the sum of the following terms:
\paragraph{1. Dirichlet prior terms:}
\[
  \log p(\pi^1 \mid \alpha^1)
  \;+\;
  \log p(\pi^2 \mid \alpha^2)
  \;+\;
  \sum_{k=1}^K \log p(\rho_k \mid \beta_k).
\]
\paragraph{2. Latent class variables \(Z^1_n, Z^2_n\) given \(\pi^1, \pi^2\):}

Each \(Z^1_n\) is categorical with parameter \(\pi^1\).  In indicator function notation:
\begin{equation*} 
\log p(Z^1_n \mid \pi^1)
\;=\; 
\sum_{d=1}^{D^1} 
  \mathbf{1}\{Z^1_n = d\}\,\log \pi^1_d.
\end{equation*}

Similarly,
\[
\log p(Z^2_n \mid \pi^2)
\;=\; 
\sum_{d=1}^{D^2} 
  \mathbf{1}\{Z^2_n = d\}\,\log \pi^2_d.
\]

\paragraph{3. Observations \(\bm{X_{nj}}\) given \(\bm{\rho_{k}}\) where \(k = Z^1_n + Z^2_n\):}
\[
\log p\!\bigl(X_{nj} \mid Z^1_n, Z^2_n, \rho\bigr)
\;=\;
\sum_{m=1}^{D^3}
  \mathbf{1}\{X_{nj} = m\}\,
  \log \rho_{\,Z^1_n + Z^2_n,\;m}.
\]

\noindent
Putting these all together, the full joint log‐probability is:

\[
\begin{aligned}
&\log p\bigl(X,\!Z^1,\!Z^2,\!\pi^1,\!\pi^2,\!\rho 
    \mid \alpha^1,\!\alpha^2,\!\beta \bigr) \\
&=
\underbrace{\log p(\pi^1 \mid \alpha^1)
\;+\;
\log p(\pi^2 \mid \alpha^2)
\;+\;
\sum_{k=1}^K \log p(\rho_k \mid \beta_k)}_{\text{Dirichlet priors}}
\\[6pt]
&\quad
+\;
\sum_{n=1}^N 
\Bigl[
  \underbrace{\sum_{d=1}^{D^1} \mathbf{1}\{Z^1_n = d\}\,\log \pi^1_{d}}_{\log p(Z^1_n \mid \pi^1)}
  \;+\;
  \underbrace{\sum_{d=1}^{D^2} \mathbf{1}\{Z^2_n = d\}\,\log \pi^2_{d}}_{\log p(Z^2_n \mid \pi^2)}
\\
&\qquad\qquad\quad
  + \sum_{j=1}^J
    \underbrace{\sum_{m=1}^{D^3}
      \mathbf{1}\{X_{nj} = m\}\,\log \rho_{\,Z^1_n + Z^2_n,\;m}}_{\log p(X_{nj} \mid Z^1_n,Z^2_n,\rho)}
\Bigr].
\end{aligned}
\]
% \addcontentsline{toc}{section}{References}
% \printbibliography{}
\end{document}

